# Why Work Like a Software Engineer? 

First of all, because everybody and their grandmothers seem to do it. Statistical computing continues to be on the rise in many branches of research. 


<!-- The below graph shows monthly accumulated extension package downloads for the R Language of Statistical Computing grouped by fields of research^[proxied this and that].
Econometrics, Psychometrics, National Language Processing, Official Statistics, Social Sciences

```{r, eval=TRUE}
# add CRAN TASK VIEW Download 
# Statistics graph here
rnorm(10)
```

-->



<div class="caption-left">Source code can be a tremendously sharp, unambiguous and international communication channel.</div>



Second because it's reproducible. Code has become a tremendous communication channel. Your web scraper does not work? 
Instead of reaching out in a clumsy but wordy cry for help, posting what you tried so far described by source code will often get you good answers within hours on platforms like [Stackoverflow](https://stackoverflow.com) or [Crossvalidated](https://crossvalidated.com). Or think of feature requests: After a little code ping pong with the package author your wish eventually becomes clearer. Let alone chats with colleagues and co-authors. Sharing code just works. 
Academic journals have found that out, too in the meantime. Many outlets require you to make the data and source code behind your work available. [Social Science Data Editors](https://social-science-data-editors.github.io/guidance/template-README.html) is a bleeding edge project at the time of writing this, but is already referred to by top notch journals like American Economic Review (AER). 

Third, because it scales and automates. Automation is not only convenient. Like when you want to download data, process and create the same visualization and put it on your website any given Sunday. Automation is inevitable. Like when you have to gather daily updates from different outlets or work through thousands of .pdfs. 

Last but not least because of things you couldn't do w/o being an absolute guru (if at all) if wasn't for programming. Take  visualization. Go, check [these D3 Examples](https://d3js.org/). Now, try to do that in Excel. If you do these things in Excel it'd make you an absolute spreadsheet visualization Jedi, probably missing out on other time consuming skills to master. Moral of the story is, with decent, carpentry level programming skills -- that'd be the upfront investment -- you can already do so many spectular things while not really specializing and staying very flexible. 



## How to Read this Book? 

The focal goal of this book is to map out the open source stack, identify neuralgic components and give you an idea of how to improve not only in programming but also in navigating the wonderful but vast open source ecosystem. Chapter 2 is the roadmap for this book: it describes and classifies the different parts of the open source stack and explains how these pieces relate to each other. Subsequent chapters highlight core elements of the open source toolbox one at a time and walk through applied examples mostly written in the R language.   

This book is a companion. The companion I wish I had when I started an empirical, data intersive PhD in economics. Yet, the book is written years after said PhD was completed and with the hindsight of 10+ years in academia. *Programming with Data* is written based on the experience of helping students and seasoned researchers of different fields with their data management, processing and communication of results.

If you are confident in your ambition to amp up your programming to at least [solid software carpentry level]() within the next few months, I suggest to get an idea of your starting point relative to this book.The upcoming *backlog* section is essentially a list of suggested to-dos  on your way to solid software carpentry. Obviously, you may have cleared a few tasks of its backlog item before even reading this boo which is fine.  On the other hand you might know about some of the things that are listed in the *requirement* section which is fine, too. The backlog and requirements mean to give you some orientation. 
 
If you do not feel committed, revisit the previous section, discuss the need for programming with peers from your domain and possibly talk to a seasoned R or Python programmer. Motivation and commitment are key for endurance needed to develop programming into a skill that truly leverage your domain specific expertise. 


## Backlog

Other than the classification and overview that *Programming with Data* provides, its introduction to **git version control** and the **collaboration workflow associated with git** are likely the most impactful items on your backlog. If you are not familiar with commits, pulls, pushes, branches, forks and pull requests, *Programming with Data* will open up a new world for you.
by introducing you to industry standard collaboration. A solid *git* foundation makes you fit into plethora of (software) teams around the globe -- in academia in beyond.

Your backlog en route to a researcher who is comfortable programming with data obviously contains a **strategy to improve your programming** itself and is complemented by a solid understanding of the **challenges of data management** from persistent storage to access restrictions. Plus, modern data driven science often has to handles datasets so large, **infrastructure** other than local desktops come into play. Yet, high performance computing (HPC) is by far not the only reason why its handy for a researcher to have a basic understanding of **infrastructure**. **Communication** of results including data dissemination or interactive online reports require the content to be served from a server with permanent online access. Basic workflow **automation** of regular procedures, e.g., for a repeated extract-transform-load (ETL) process to update data, is a low hanging (and very useful) fruit for a programming researcher. 

The case studies at the end of the book are not exactly a backlog item like the above but still a recommended read. The case studies in this book are hands-on programming examples -- mostly written in R -- to showcase tasks from API usage to geospatial visualization in reproducible fashion. Reading and actually running other developer's code does not only improve one's own code, but helps to see what makes code inclusive and what hampers comprehensibility. 



## Requirements

Essentially, the requirements in this section just extend the list of backlog items suggested above with a few things not covered in this book. This does not mean that every single item is strictly required as a prerequisite to make sense of this *Programming with Data*. The requirments are more to say that 

- initial steps with a scripting language such as R or Python
- console / terminal basics (moving around the filesystem, ssh)

are explained more comprehensively elsewhere: ([Big Book of R](), [R for Data Science](), [The Carpentries]())  



like backlog, but rather before you should go on this journey. 

What You Need to Know to Make the Most of This Book

Though the book offers many entry points and strives to make advanced considerations accessible, 
*Programming with Data* provides the most value for readers whose prior knowledge is beyond certain threshold. The book is **not** an introduction to R nor Python. It's **not** an introduction to data science or machine learning either. Neither knowledge is strictly required to follow the book, but a basic understanding of statistical methods and the challenges applied statisticians face certainly helps to motivate getting invested into programming. Likewise, prior knowledge of a *scripting language* like *R, Python* or *Javascript*, *git version control*, as well as familiarity with  *console / terminal basics* will help the reader sail smoothly. Still though, to self-reflect on one's starting point remains the most important requirement to leverage the book. 

*Programming with Data* willingly accepts to be overwhelming at times. Given the variety of topics touched in an effort to show the big picture, I encourage the reader to remain relaxed about a few blanks even when it comes to fundamentals. The open source community offers plenty of great resources to selectively upgrade skills and this books intends to show how to evaluate the need for help and how to find the right sources. 

*Programming with Data* was written with the idea in mind that well maintained open source projects are the best to document themselves. Therefore I rather reference to existing documentation as opposed to rewriting it. Because even if my wording was better than the original, it is only a matter of time before my documentation is outdated. The bigger the stack the harder to keep track. So whenever, you stumble upon something you do not understand, make sure to look it up. (I tried hard to provide good a glossary and reference wherever possible, but if not - still push yourself to look things up). 

That being said, to have an idea about following technologies and ideas will certainly help to get the most out of spending time with *Programming with Data*: 

- Basic R or Python 
- basic understanding of git version control 
- basic understanding of GitHub based workflows, e.g., issue tracker, kanban boards... 
- Terminal / Console and Shell and a shell, e.g., bash, fish

In other words: if none of the above means something to you, I recommend to make yourself comfortable with the basics of 2-3 of the above fields before you start to read this book or just cherry pick single topics.




## Backlog

What you need to know to become comfortable programming with data... 








