[
["preface.html", "Hacking for Social Sciences A Guide to Programming With Data 1 Preface", " Hacking for Social Sciences A Guide to Programming With Data Matthias Bannert 2021-10-06 1 Preface The vast majority of data has been created within the last decade. In turn many fields of research are confronted with an unprecented wealth of data. The sheer amount of information but also the complexity of modern datasets continues to point a kind researcher to programming approaches who had not considered programming to process data so far. Hacking for Social Sciences aims to give a big picture overview and starting point to reach what the open source software community calls a ‚Äòsoftware carpentry‚Äô level. Also, this book argues a solid software carpentry skill level is totally in reach for most researchers. And most importantly, investing is worth the effort: being able to code leverages field specific expertise and fosters interdisciplinary collaboration as source code continues to become an important communication channel. Meet Dr.¬†Egghead who started his quest to figure out how his assistant got a week‚Äôs work done in two hours. ‚ÄúHacker‚Äôs wear hoodies, you know‚Äù, \" he mumbles as he starts to think1. FWIW, Dr.¬†Egghead was born after Colin Gillespie‚Äôs hilarious talk on Security in Academia at useR! 2019‚Ü©Ô∏é "],
["introduction-the-choice-that-doesnt-matter.html", "2 Introduction - The Choice that Doesn‚Äôt Matter 2.1 Why Would Social Scientists Want to Code? 2.2 How to Read this Book?", " 2 Introduction - The Choice that Doesn‚Äôt Matter The very first (and intimidating) choice a novice hacker faces is which is programming language to learn. Unfortunately the medium popularily summed up as the internet offers a lot of really really good advice on the matter. The problem is, however, that this advice does not necessarily agree which language is the best for research. In the realm of data science ‚Äì get accustomed to that label if you are a scientist who works with data ‚Äì the debate basically comes down to two languages: The R Language for Statistical Computing and Python. At least to me, there is only one valid advice: It simply does NOT matter. If you stick around in data science long enough you will eventually get in touch with both languages and in turn learn both. There is a huge overlap of what you can do either of those languages. R came out of the rather specific domain of statiscs 25+ years ago and made its way to a more general programming language thanks to 15K+ extension packages (and counting). Built by a mathmatician, Python continues to be as general purpose as it‚Äôs ever been. But it got more scientific, thanks to extension packages of its own such as pandas, SciPy or numPy. As a result there is a huge overlap of what both languages can do and both will extend your horizon in unprecendented fashion if you did not use a full fledged programming language for your analysis before. R: ‚ÄúDplyr smokes pandas.‚Äù Python: ‚ÄúBut Keras is better for ML!‚Äù Language wars can be entertaining, sometimes spectacular, but most times they are just useless‚Ä¶ But why is there such a heartfelt debate online, if it doesn‚Äôt matter? Let‚Äôs pick up a random argument from this debate: R is easier to set up and Python is better for machine learning. If you worked with Java or another environment that‚Äôs rather tricky to get going, you are hardened and might not cherish easy onboarding. If you got frustrated before you really started, you might feel otherwise. You may just have been unlucky making guesses about a not so well documented paragraph, trying to reproduce a nifty machine learning blog post. Just because you installed the wrong version of Python or didn‚Äôt manage to make sense of virtualenv right from the beginning. The point is, rest assured, if you just start doing analytics using a programming languages both languages are guaranteed to carry you a long way. There is no way to tell for sure which one will be the more dominant language in 10 years from now or whether both still be around holding their ground the way they do now. But once you reached a decent software carpentry level in either language, it will help you a lot learning the other. If your peers work with R, start with R, if your close community works with Python, start with Python. If you are in for the longer run either language will help you understand the concepts and ideas of programming with data. Trust me, there will be a natural opportunity to get to know the other. 2.1 Why Would Social Scientists Want to Code? First of all, because everybody and their grandmothers seem to do it. Statistical computing continues to be on the rise in many branches of social sciences. Source code can be a tremendously sharp, unambigous and international communication channel. Second because it‚Äôs reproducible. Code has become a tremendous communication channel. Your web scraper does not work? Instead of reaching out in a clumsy but wordy cry for help, posting what you tried so far described by source code will often get you good answers within hours on platforms like Stackoverflow or Crossvalidated. Or think of feature requests: After a little code ping pong with the package author your wish eventually becomes clearer. Let alone chats with colleagues and co-authors. Sharing code just works. Academic journals have found that out, too in the meantime. Many outlets require you to make the data and source code behind your work available. Social Science Data Editors is a bleeding edge project at the time of writing this, but is already referred to by top notch journals of the profession like American Economic Review (AER). Third, because it scales and automates. Automation is not only convenient. Like when you want to download data, process and create the same visualization and put it on your website any given Sunday. Automation is inevitable. Like when you have to gather daily updates from different outlets or work through thousands of .pdfs. Last but not least because of things you couldn‚Äôt do w/o being an absolute guru (if at all) if wasn‚Äôt for programming. Take visualization. Go, check these D3 Examples. Now, try to do that in Excel. If you do these things in Excel it‚Äôd make you an absolute spreadsheet visualization Jedi, probably missing out on other time consuming skills to master. Moral of the story is, with decent, carpentry level programming skills ‚Äì that‚Äôd be the upfront investment ‚Äì you can already do so many spectular things while not really specializing and staying very flexible. 2.2 How to Read this Book? Hacking for Social Sciences is written based on the experience of helping students and seasoned researchers of different fields with their data management, processing and communication of results. A part of the book contains the information I wish I had when I started a PhD in economics. Part of the book is written years after said PhD was completed and with the hindsight of 10+ years in academia. Every page of the book is written with the belief that the future is OPEN and it is up to our generation of researchers to shape it. ‚ÄúThe ministry warns: The future is open,‚Äù taken from a 2020 ad campaign on Open Access by the German ministry for education and research (pdf). If you came to üçí pick, you‚Äôre welcome, too (but a bit early to the party though). This book will grow along the 2020 course ‚ÄòHacking for Social Science‚Äô and hopefully be finished in its first version by the end of the semester / year. Next up are chapters on the Big Picture of Open Source Software for hacking data and Git version control. "],
["stack-a-developers-toolkit.html", "3 Stack - A Developer‚Äôs Toolkit 3.1 Languages: Compiled vs.¬†Interpreted 3.2 IDE 3.3 Version Control 3.4 Database: Relational vs.¬†Non-Relational 3.5 Environments: Where Should I Run Stuff ? 3.6 Communication 3.7 Automation", " 3 Stack - A Developer‚Äôs Toolkit Just like natural craftsmen, digital carpenters depend on their toolbox and their mastery of it. Stack is what developers call the choice of tools used in a particular project. Even though different flavors come down to personal preferences, there is a lot of common ground in programming with data stacks. Throughout this book, often a choice for one piece of software needs to be made in order to illustrate things. But please notice that these choices are examples and focus on the role of an item in the big picture. To help you with the big picture of which tool does what, the following section will group common programming-with-data stack components. Also, notice that not every role has to be filled in every project. Aaaaaaah! Don‚Äôt panic, Dr.¬†Egghead! All these components are here to help you and you won‚Äôt need all of them every from the start. Here are some components I use most often. This is a personal choice which works for me. Obviously not ALL of these are components are used in every small project. Git, R and R Studio would be my very minimal version. Component Choice Interpreter / Language R, Python, Javascript IDE / Editor R Studio,VS Code, Sublime Version Control Git Project Management GitHub, GitLab Database PostgreSQL ‚ÄòVirtual‚Äô Environments Docker Communication (Visualization, Web) Node, Quasar (vue.js) Website Hosting Netlify, GitHub Pages Workflow Automation Apache Airflow Continous Integration GitLab CI 3.1 Languages: Compiled vs.¬†Interpreted In Statistical Computing ‚Äì at least in Social Sciences ‚Äì the interface between the researcher and the computation node is almost always an interpreted progamming language as opposed to a compiled one. Compiled languages like C++ require the developer to write source code and compile, i.e., translate source code into what a machine can work before runtime. The result of the compilation process is a binary which is specific to the operating system. Hence you will need one version for Windows, one for OSX and one for Linux if you intend to reach a truly broad audience with your program. The main advantage of a compiled language is speed in terms of computing performance because the translation into machine language does not happen during runtime. A reduction of development speed and increase in required developer skills are the downside of using compiled languages. Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. ‚Äì Dan Ariely, Professor of Psychology and Behavioral Economics, on twitter The above quote became famous in the hacking data community, not only because of the provocative, fun part of it, but also because of the implicit advice behind it. Given the enormous gain in computing power in recent decades, but also methodological advances, interpreted languages are often fast enough for many social science problems. And even if it turns out, your data grow out of your setup, a well written proof of concept written in an interpreted language can be a formidable blueprint. Source code is turning into an important scientific communication channel. Put your money on it, your interdisciplinary collaborator from the High Performance Computing (HPC) group, will prefer some Python code as a briefing for their C++ or FORTRAN program over a wordy description out of your field‚Äôs ivory tower. Interpreted languages are a bit like pocket calculators, you can look at intermediate results, line by line. R and Python are the most popular OSS choices in hacking with data, Julia is an up and coming, perfomance focused language with a much slimmer ecosystem. A bit of Javascript can‚Äôt hurt for advanced customization of graphics and online communication of your results. 3.2 IDE It‚Äôs certainly possible to move a five person family into a new home by public transport, but it is not convenient. The same holds for (plain) text editors in programming. You can use them, but most people would prefer an Integrated Development Environment (IDE) just like they prefer to use a truck when they move. IDEs are tailored to the needs and idiosyncrasies of a language, some working with plugins and covering multiple languages. Others have a specific focus on a single language or a group of languages. Here are some of the features you are looking for in an IDE for programming with data: code highlighting, linting decent file explorer terminal integration git integration markdown support debugging tools build tools customizable through add-ins / macros For R, the Open Source Edition of R Studio Desktop is the right choice for most people. (If you are working in a team, R Studio‚Äôs server version is great. It allows to have a centrally managed server which clients can use through their a web browser without even installing R and R Studio locally.) R Studio has solid support for a few other languages often used together with R, plus it‚Äôs customizable. The French premier thinkR Colin_Fay gave a nice tutorial on Hacking R Studio at the useR! 2019 conference. Screenshot of the coverpage of the R Studio website in fall 2020. The site advertises R Studio as an IDE for both languages R and Python. Remember The Choice that Doesn‚Äôt Matter? While R Studio managed to hold its ground among R aficionados as of fall 2020, Microsoft‚Äôs free Visual Studio Code has blown the competition out of the water otherwise. Microsoft‚Äôs IDE is blazing fast, extendable and polyglot. VS Code Live Share is just one rather random example of its remarkably well implemented features. Live share allows developers to edit a document simultaneously using multiple cursors in similar fashion to Google Docs, but with all the IDE magic. And in a Desktop client. Another approach is to go for a highly customizable editor such as Sublime or Atom. The idea is to send source code from the editor to interchangeable REPLs which can be adapted according to the language that needs to be interpreted. That way a good linter / code highlighter for your favorite language is all you need to have a lightweight environment to run things. An example of such a customization approach is Christoph Sax‚Äô small project Sublime Studio. Other examples for popular IDEs are Eclipse (mostly Java but tons of plugins for other languages), or JetBrains‚Äô IntelliJ (Java) and PyCharm (Python). 3.3 Version Control To buy into the importance of managing one‚Äôs code professionally may be the single most important take away from Hacking for Social Sciences. Being able to work with version control will help you fit into a lot of different teams that have contact points with data science and programming, let alone if you become part of a programming or data science team. While version control has a long history dating back to CVS and SVN, the good news for the learner is, that there is a single dominant approach when it comes to version control in academia. Despite the fact that its predecessors and alternatives such as mercurial are stil around, git is the one you have to learn. To learn more about the history of version controls and approaches other than git, Eric Sink‚Äôs Version Control by Example is for you. So what does git do for us as researchers? How is it different from dropbox? git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. git does not work like dropbox. The idea of thinking of a sync, is what interferes with comprehension of the benefit of version control (which why I hate that git GUIs call it ‚Äòsync‚Äô anyway to avoid irritation of user‚Äôs initial believes.). Git is a decentralized version control system that keeps track of a history of semantic commits. Those commits may consist of changes to multiple files. A commit message summarizes the gist of a contribution. Diffs allow to compare different versions. The diff output shows an edit during the writing of this book. The line preceeded by ‚Äò-‚Äô was replaced with the line preceeded by ‚Äò+.‚Äô Git is well suited for any kind of text file. May it be source code from Python or C++, or some text written in markdown or LaTeX. Binaries like .pdfs or Word documents are possible, too, but certainly not the type of file for which git is really useful. This book contains a detailed, applied introduction tailed to researchers as part of the Programmers‚Äô Practices and Workflows chapter, so let‚Äôs dwell with the above contextualization for a bit. 3.4 Database: Relational vs.¬†Non-Relational To evaluate which database to pick up just seems like the next daunting task of stack choice. Luckily, in research first encounters with a database are usually passive, in the sense that you want to query data from a source. In other words the choice has been made for you. So unless you want to start your own data collection from scratch, simply sit back, relax and let the internet battle out another conceptual war. Database Management Systems (DBMS) are basically grouped into relational and non-relational ones. Relational databases come with the Structured Query Language (SQL) and have been around forever. SQL became and ISO and ANSI standard and continues to be the essence of many many backends around the world. Oracle continues to be the benchmark for SQL databases but the opensource PostgreSQL and Microsoft‚Äôs SQL Server operate at eye level for many applications. MySQL, Oracle‚Äôs slim, little (but free) brother, can‚Äôt quite cope with PostgreSQL, continues to be the most used SQL database on the planet. This is mainly due to its popularity for web applications like the blogging CMS wordpress. Last but not least, sqlite needs to be mentioned when talking about relational database. Sqlite is built into all mobile phones and most computers. Its name nutshells its concept quite well: sqlite is an easy to use, much simpler version of the aforementioned database management systems. It is extremely popular for light but powerful applications that organize data with a SQL approach in a single file. No-SQL databases are the anti establishment, anti standard approach. MongoDB may be the best marketed among the rebels. Before you start to sympathize with latter approach because the wording of my last to sentences, let‚Äôs stop here. Large infrastructure players make the case for non-relational stores like CouchDB or Amazon Redshift Database, but trust me, those are unlikely the first things you get to run when your research grows out of Excel. If your are not happy with the ‚Äòbeyond-the-scope-of-this-book‚Äô argument, blogging experts like Lukas Eder maybe biased but much better educated to educate you here. The idea of this chapter is just to help you group all the database management systems you might face soon as a researchers. The good news is, languages like R and Python are so well equipped to interface with a plethora of databases. So well, that I often recommend these languages to researchers who work with other less equipped tools, solely as an intermediate layer. And if there is really no database extension around for your language, a general ODBC interface helps ‚Äì at least for SQL databases. 3.5 Environments: Where Should I Run Stuff ? But what if you do not want to run your stuff locally? Maybe because you want to expose a web application or site which has to be online 24/7. Maybe because you do not more additional computing power. In principle there are four options. Bare Metal. Buy hardware, hook it up with the internet, install your software, have fun. If you have a consistantly high usage rate and can maintain the server efficiently, this is likely the cheapest computing hour. You saw that if, didn‚Äôt you? Either use physical or virtual hardware and make your local IT department host your service on-premise (inhouse). You will give up a little bit of your flexible in exchange for help with maintenance and server administration. Cloud Computing. Software-as-a-Service 3.6 Communication Communication is an essentially part of building an (academic) career. Part of it is a neat online profile. Do not relax on the excuse that your department‚Äôs website does not offer the flexibility. The legal and technical situation in many places should allow you to spin up your own website or even run a blog if you find the time. For free. Including the web hosting. A popular approach to do so is to work with a static website generator. Generators like blogdown, pkgdown or bookdown are flavors of the same approach to create a website: Write markdown first, render it and then upload rendered HTML + CSS + Javascript to a host like GitHub Pages or Netlify that allow you host your site for free. The static website generator approach has become so popular that the aforementioned hosters even offer to run the render process for you. The idea of engines like the Go based Hugo or the Ruby based Jekyll which are behind the above packages is a counter approach to what content management systems do: There is no database or template that is brought together dynamically when a user visits the website. The rendering is done locally on the creator‚Äôs local computer (or netlify‚Äôs environment). Whenever a change is made, the website is rendered entirely (ok, minus caching) and uploaded (pushed) again to the host. Therefore no database is needed which cuts down the costs of hosting to zero. (FWIW: this book is made with such a generator.) 3.7 Automation The first type of automation described here refers to automation of your development workflow. That it is, you standardize your path from draft to program to deployment to production. Modern version control software accompanies this process with a toolchain that is often fuzzily called CI/CD. While CI stands for continuous integration and simply refers to a workflow in which the team tries to release new features to production as continuously as possible, CD stands for either continuous delivery or continuous deployment. However, in practice the entire toolchain referred to as CI/CD has become broadly available in well documented fashion when git hosting powerhouses GitHub and GitLab introduced their flavors of it: [GitHub Actions])(https://docs.github.com/en/actions) and GitLab CI. In addition services like Travis CI or Circle CI offer this toolchain independently of hosting git repositories. Users of these platforms can upload a simple textfile that follows a name convention and structure to trigger a step based toolchain based on an event. An example of an event may be the push to a repository‚Äôs main branch. A common example would be to run tests and/or build a package and upon success deploy the newly created package to some server ‚Äì all triggered by simple push to master. One particularly cool thing is, that there multiple services who allow to run the testing on their servers using container technologies. This leads to great variety of setups for testing. That way software can easily be tested on different operating systems / environments. Also the mentioned website rendering approach mentioning in the previous section as a potential CI/CD application. Here is a simple example of a .gitlab-ci.yml configuration that builds and tests a package and deploys it. It‚Äôs triggered on push to master: stages: - buildncheck - deploy_pack test: image: name: some.docker.registry.com/some-image:0.2.0 entrypoint: - &quot;&quot; stage: buildncheck artifacts: untracked: true script: - rm .gitlab-ci.yml # we don&#39;t need it and it causes a hidden file NOTE - install2.r --repos custom.mini.cran.ch . - R CMD build . --no-build-vignettes --no-manual - R CMD check --no-manual *.tar.gz deploy_pack: only: - master stage: deploy_pack image: name: byrnedo/alpine-curl entrypoint: [&quot;&quot;] dependencies: - &#39;test&#39; script: - do some more steps to login and deploy to server ... For more in depth examples of the above, Jim Hester‚Äôs talk on GitHub Actions for R is a very good starting point. The other automation tool I would like to mention is Apache Airflow because of its ability to help researchers keep an overview of regularly running processes. Examples of such processes could be daily or monthly data sourcing or timely publication of a regularly published indicator. I often referred to it as cronjobs on steroids. Airflow ships with a dashboard to keep track of many timed processes, plus a ton of other log and reporting features worth a lot when maintaining reocurring processes. "],
["developer-practices-workflows.html", "4 Developer Practices &amp; Workflows 4.1 Peer Programming 4.2 RSA Key Pair Authentication 4.3 Git Version Control 101 4.4 Feature Branches, PRs and Forks 4.5 Project Management Basics 4.6 Plan Your Program 4.7 Naming Conventions: Snake, Camel or Kebap 4.8 Folder Structure", " 4 Developer Practices &amp; Workflows Just like most experienced engineers, seasoned software developers follow some kind of school or paradigm. Good programmers can even switch among approaches according to their current project‚Äôs needs or depending on the team they are on. This section does not want to give a comprehensive overview over programming concepts nor compare approaches. And damn sure it does not mean to go to war over approach superiority. Hacking for Social Scientists rather cherry-picks suitable application-minded, low-barrier concepts that help social scientists professionalize their own programming. 4.1 Peer Programming Peer programming, also called pair programming just means two developers sit in front of the same screen to collaborate on a piece of code. So why is there such a buzz about it? Why is there even a term for it? And why is there a section in an applied book on it? That is because novice programmers (and their scientific supervisors) often doubt the efficiency of two paid persons working at the same work station. But programming is not about digging a hole with two shovels. Particularly not when it comes to building the software basis or frame of a project. Working together using one single keyboard and screen or the virtual equivalent thereof can be highly efficient. The virtual equivalent, i.e., in fact using two computers but sharing the screen while in call, helps tremendously with a) your concept, b) your documentation. Plus, it is a code review at the same time. But most importantly both developers learn from each other. Having to explain and being challenged, deepens the understanding of experienced developers and ruthlessly identifies holes in one‚Äôs knowledge. One important advice when peer programming is to switch the driver‚Äôs seat from time to time. Make sure the lesser programmer holds the keys from time to time and maneuvers through articles, repositories, code and data. Doing so prevents the co-pilot from taking a back seat and letting the veteran do the entertainment. Visual Studio Code Live Share is a great feature for next level virtual peer programming as it allows for two drivers using two cursors. Of course there are downsides of the pair programming approach, too. Also, timing within the lifecycle of a project is an important factor and not every project is the same fit for this agile method. But given there are so many approaches, I will leave the back and forth to others. The goal of this section is to point the reader to a practical approach that tends to work well in programming with data setups in social sciences. Googlers Jeff Dean and Sanjay Ghemawat had its fair of success, too, according to the New Yorker‚Äôs https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge. 4.2 RSA Key Pair Authentication This section could be headed ‚Äòlog in like a developer.‚Äô RSA Key Pairs are a convenient, relatively secure way to log into an account. SSH based connections, including secure copy (SCP), often make use of RSA Key Pairs instead of using a combination of username and password. Also, most git platforms use this form of authentication. The basic idea of key pairs is to have a public key and a private key. While the private key is never shared with anyone, the public key is shared with any server you want to log in to. It‚Äôs like getting a custom door for any house that you are allowed to enter: share your public key with the server admin / or web portal and you‚Äôll be allowed in when you show your private key. In case you loose your private key or suspect it has been stolen, simply inform the admin, so she can remove the door (the public key). This is were a little detail comes into play: you can password protect the authentication process. Doing so buys you time to remove the key from the server before your password gets bruteforced. The downside of this additional password is its need for interaction. So when you are setting up a batch that talks to a remote server that is when you do not want a key /w password. Step one en route to log in like a grown up, is the create an RSA key pair. Github has a 1-2-3 type of manual to get it done. Nevertheless, I would like the R Studio (Server) specific way here. Login to https://teaching.kof.ethz.ch/ (or use your local R Studio Desktop) Go to Tools -&gt; Global Options -&gt; Git/SVN Hit Create RSA KEY (When you some crazy ASCII art reminiscant of a rabbit, it‚Äôs just ok.) Click ‚ÄòView Public Key‚Äô Copy this key to the your clipboard. The R Studio GUI is an easy way to create an RSA Key Pair. You can paste the key you obtained to your Github settings or put it into your server‚Äôs authorized keys file. Congrats you may log in now! 4.3 Git Version Control 101 As stated before, version control may be the single most important thing to take away from Hacking for Social Sciences. In this chapter about the way developers work, I will stick to version control with git. The stack discussion of the previous chapter features a few more version control systems, but given git‚Äôs dominant position, we will stick solely to git in this introduction to version control. 4.3.1 What is Git Version Control? Git is a decentralized version control system. It manages different versions of your source code (and other text files) in a simple but efficient manner that has become the industry standard: The git programm itself is a small console programm that creates and manages a hidden folder inside the folder you put under version control (you know those folders with a leading dot in their foldername, like .myfolder). This folder keeps track of all differences between the current version and other versions before the current one. Meaningful commit messages help to make sense of a project‚Äôs history. The key to appreciate the value of git is to appreciate the value of semantic versions. Git is not Dropbox nor Google Drive. It does not sync automagically (even if some Git GUI Tools suggest so). Because these GUIs tools2 may be convenient but do not really help to improve your understanding of the workflow, we will use the git console throughout this book. As opposed to the sync approaches mentioned above, a version control system allows to summarize a contribution across files and folders based on what this contribution is about. Assume you got a cool pointer from an econometrics professor at a conference and you incorporated her advice in your work. That advice is likely to affect different parts of your work: your text and your code. As opposed to syncing each of these files based on the time you saved them, version control creates a version when you decide to bundle things together and to commit the change. That version could be identified easily by its commit message ‚Äòincorporated advice from Anna (discussion at XYZ Conf 2020).‚Äô 4.3.2 Why Use Version Control in Research? A version control based workflow is a path to your goals that rather consists of semantically relevant steps instead of semantically meaningless chunks based on the time you saved them. In other more blatant, applied words: naming files like final_version_your_name.R or final_final_correction_collaboratorX_20200114.R is like naming your WiFi dont_park_the_car_in_the_frontyard or be_quiet_at_night to communicate with your neighbors. Information is supposed to be sent in a message, not a file name. With version control it is immediately clear what the most current version is - no matter the file name. No room for interpretation. No need to start guessing about the delta between the current version and another version. Also, you can easily try out different scenarios on different branches and merge them back together if you need to. Version control is a well established industry standard in software development. And it is relatively easy to adopt. With datasets growing in size and complexity, it is only natural to improve management of the code that processes these data. Academia has probably been the only place that would allow you to dive into hacking at somewhat complex problems for several years w/o ever taking notice of version control. As a social scientist who rather collaborates in small groups and writes moderate amount of code, have you ever thought about how to collaborate with 100+ persons in a big software project? Or to manage ten thousands of lines of code and beyond? Version control is an important reason why these things work. And it‚Äôs been around for decades. But enough about the rant‚Ä¶ 4.3.3 How Does Git Work ? This introduction tries narrow things down to the commands that you‚Äôll need if want to use git in similar fashion to what you learn from this book. If you are looking for more comprehensive, general guides, three major git platforms, namely Atlassian‚Äôs Bitbucket, GitHub and Gitlab offer comprehensive introductions as well as advanced articles or videos to learn git online. The first important implication of decentralized version control is that all versions are stored on the local machines of every collaborator, not just on a remote server (this is also a nice, natural backup of your work). So let‚Äôs consider a single local machine first. Locally, a git repository consists of a checkout which is also called current working copy soon. This is the status of the file that your file explorer or your editor will see when you use them to open a file. To checkout a different version, one needs to call a commit by its unique commit hash and checkout that particular version. If you want to add new files to version control or bundle changes to some existing files into a new commit, add these files to the staging area, so they get committed next time a commit process is triggered. Finally committing all these staged changes under another commit id a new version is created. 4.3.4 Moving Around So let‚Äôs actually do it. Here‚Äôs a three stage walk through of git commands that should have you covered in most use cases a researcher will face. Note that git has some pretty good error message that guess what could have gone wrong. Make sure to read them carefully. Even if you can‚Äôt make sense of them, your online search will be a lot more efficient when you include these messages. Stage 1: Working Locally Command Effect git init put current directory and all its subdirs under version control. git status shows status git add file_name.py adds file to tracked files git commit -m ‚Äòmeaningful msg‚Äô creates a new version/commit out of all staged files git log show log of all commit messages on a branch git checkout some-commit-id go to commit, but in detached HEAD state git checkout main-branch-name leave temporary state, go back to last commit Stage 2: Working with a Remote Repository Though git can be tremendously useful even without collaborators, the real fun starts when working together. The first step en route to get others involved is to add a remote repository. Command Effect git clone creates a new repo based on a remote one git pull get all changes from a linked remote repo git push deploy all commit changes to the remote repo git fetch fetch branches that were created on remote git remote -v show remote repo URL git remote set-url origin https://some-url.com set URL to remote repo Stage 3: Branches Branches are derivatives from the main branch that allow to work on different feature at the same time without stepping on someone elses feet. Through branch repositories can actively maintain different states. Command Effect git checkout -b branchname create new branch named branchname git branch show locally available branches git checkout branchname switch to branch named branchname git merge branchname merge branch named branchname into current branch Fixing Merge Conflicts In most cases git is quite clever and can figure out which is the desired state of a file when putting two versions of it together. When git‚Äôs recursive strategy is possible, git it wil merge versions automatically. When the same lines were affected in different versions, git cannot tell which line should be kept. Sometimes you would even want to keep both changes. But even in such scenario fixing the conflict is easy. Git will tell you that your last command caused a merge conflict and which files are conflicted. Open these files and see all parts of the file that are in question. Ouch! We created a conflict by editing the same line in the same file on different branches and trying to but these branches back together. Luckily git marks the exact spot where the conflict happens. Good text editors / IDEs ship with cool colors to highlight all our options. go for the current status or take what‚Äôs coming in from the a2 branch? Some of the fancier editors even have git conflict resolve plugins that let you walk through all conflict points. In VS Code you can even click the option. At the and of the day, all do the same, i.e., remove the unwanted part including all the marker gibberish. After you have done so, save, commit and push (if you are working with a remote repo) . Don‚Äôt forget to make sure you kinked out ALL conflicts. 4.4 Feature Branches, PRs and Forks This section discusses real world collaboration workflows of modern open source software developers. Hence the prerequisites are a bit different in order to benefit the most from this section. Make sure you are past being able to describe and explain git basics, be able to create and handle your own repositories. If you had only a handful of close collaborators so far, you may be fine with staying on the main branch and trying not to step on each others feet. This is reasonable because it is not useful to work asynchronously on exact the same lines of code anyway. Nevertheless, there is a reason why feature-branch-based workflows became very popular among developers: Imagine you collaborate less synchronously, maybe with someone in another timezone. Or with a colleague who works on your project, but in a totally different month during the year. Or, most obviously, we someone you have never met. Forks and feature-branch-based workflows is the way a lot of modern open source projects are organized. Forks are just a way to contribute via feature branches even in case you do not have write access to a repository. But let‚Äôs just have look at the basic case in which you are part of the team first. Assume there is already some work done, some version of the project is already up on GitHub. You join as a collaborator and are allowed to push changes now. It‚Äôs certainly not a good idea to simply add things without review to a project‚Äôs production. Like if you got access to modify the institute‚Äôs website and you made your first changes and all of a sudden the website looks like this: It used to be subtle and light gray. I swear! Bet everybody on the team took notice of the new team member by then. In a feature branch workflow you would start from the latest production version. Remember, git is decentralized and you have all versions of your team‚Äôs project. Right at your fingertips on your local machine. Create a new branch named indicative of the feature you are looking to work on. git checkout -b colorways You are automatically being switched to the freshly created branch. Do your thing now. It could be just a single commit, or several commits by different persons. Once you are done, i.e., commited all changes, add your branch to the remote repository by pushing. git push -u origin colorways This will add a your branch called colorways to the remote repository. If you are on any major git platform with your project, it will come with a decent web GUI. Such a GUI is the most straight forward way to do the next step: get your Pull Request (PR) going. Github pull request dialog: Select the pull request, choose which branch you merge into which target branch. As you can see, git will check whether it is possible to merge automatically w/o interaction. Even if that is not possible, you can still issue the pull request. When you create the request you can also assign reviewers, but you could also do so at a later stage. Note, even after a PR was issued you can continue to add commits to the branch about to be merged. As long as you do not merge the branch through the Pull Request, commits are added to the branch. In other words your existing PR gets updated. This is a very natural way to account for reviewer comments. Pro-Tipp: Use commit messages like ‚Äòadded join to SQL query, closes #3.‚Äô The key word ‚Äòcloses‚Äô or ‚Äòfixes,‚Äô will automatically close issues referred to when merged into the main branch. Once the merge is done, all your changes are in the main branch and you and everyone else can pull the main branch that now contains your new feature. Yay! 4.5 Project Management Basics The art of stress free productivity as I once called it in ‚Äô10 blog post, has put a number of gurus on the map and whole strand of literature to our bookshelves. So rather than adding to that, I would like to extract a healthy, best-of-breed type of dose here. The following few paragraphs do not intend to be comprehensive ‚Äì not even for the scope of software projects, but inspirational. In tje software development startup community, the waterfall approach became synonym to conservative, traditional and ancient: Overspecification in advance of the project, premature optimization and a lawsuit over expectations that weren‚Äôt met. Though waterfall project may be better than their reputation and specifications should not be too detailed and rigid. Many software projects are rather organized in agile fashion with SCRUM and KANBAN being the most popular derivatives. Because empirical academic projects have a lot in common with software projects inasmuch that there is a certain expectation and quality control, but the outcome is not known in advance. Essentially in agile project management you roughly define an outcome along the lines of a minimum viable product (MVP). That way you do not end up with nothing after a year of back and forth. During the implementation you‚Äôd meet regularly, let‚Äôs say every 10 days, to discuss development since the last meet and what short term plans for the next steps ahead. The team picks splits work into task items on the issue tracker and assigns them. Solution to problems will only be sketched out and discussed bilaterally or in small groups. By defining the work package for only a short timespan, the team stays flexible. In professional setups agile development is often strictly implemented and makes use of sophisticated systems of roles that developers and project managers can get certified for. Major git platforms ship with a decent, carpentry level project management project management GUI. The issue tracker is at the core of this. If you use it the minimal way, it‚Äôs simply a colorful to-do list. Yet, with a bit of inspiration and use of tags, comments and projects, an issue tracker can be a lot more The Github issue tracker (example from one of the course‚Äôs repositories) can be a lot more than a todo list. Swimlanes (reminiscant of a bird‚Äôs eye view of an Olympic pool) can be thought of columns that you have to walk through from left to right: To Do, Doing, Under Review, Done. (you can also customize the number and label of lanes and event associate actions with them, but let‚Äôs stick to those basic lanes in this section.) The idea is to use to keep track of the process and making the process transparent. GitHub‚Äôs web platform offers swimlanes to keep a better overview of issues being worked on. ```{, type=‚Äònote‚Äô} Tipp: No lane except ‚ÄòDone‚Äô should contain more than 5-6 issues. Doing so prevents clogging the lanes at particular stage which could potentially lead to negligent behavior, e.g., careless reviews. ## Testing When talking about development practices, testing can&#39;t be missing. So, just you know that I thought of this, tbc ... &lt;!--chapter:end:index.Rmd--&gt; # Programming 101 &lt;!-- DO NOT FORGET TO SAY SOMETHIN ABOUT LICENSES !! --&gt; If you associate programming more often than not with hours of fiddling, tweaking and fighting to galvanize approaches found online, this chapter is for you. Don&#39;t expect lots of syntax. If you came for examples of useful little programs from data visualization to parallel computing, check out the [case studies](). The following sections share a blueprint to go from explorative script to production ready package. Organise your code and accompany the evolution of your project: start out with experiments, define your interface, narrow down to a proof of concept and scale up. Hopefully the tips, tricks and the guidance in this chapter will help you to experience the rewarding feeling of a software project coming together like a plan originated by Hannibal Smith. &lt;!-- - maybe overview sketch here... decision tree. --&gt; ## Think library! &lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;images/packages.jpg&quot; height=&quot;300px&quot;&gt; &lt;div class=&quot;caption-half&quot;&gt;&quot;Over the days are when creating packages for gurus was only.&quot;&lt;/div&gt; &lt;/div&gt; The approach that I find practical for applied, empirical research projects involving code is: think library. Think package. Think _reusable_ code. Don&#39;t think you can&#39;t do it. Let me de-mystify packages for you: **Packages are nothing else than source code organized in folders following some convention**. Thanks to modern IDEs, it has never been easier to stay inline with conventions. Editors like R Studio ship with built-in support to create package skeletons with a few clicks. Thousands of open source extension packages allow you to learn from their structure. Tutorials like [Packing Python Projects](https://packaging.python.org/tutorials/packaging-projects/) or Hadley Wickham&#39;s free online book [R Packages](https://r-pkgs.org/) explain how to create packages good enough to make the official PyPi or CRAN package repository. In other words, it is unlikely that someone with moderate experience comes up the best folder structure ever invented. Sure, every project is different and not every aspect (folder) is needed in every project. Nevertheless, there are well established blueprints, guides and conventions that suit almost any project. Unlike Office type of projects which center around one single file, understand a research project will live in a folder with many subfolders and files. Not in one single file. Trust me on this one: The package approach will pay off early. Long before you ever thought about publishing your package. Write your own function definition, rather than just calling functions line by line. Write code as if you need to make sure it runs on another computer. Write code as if you need to maintain it. Go from scripts like this ```r # This is just data from the sake of # reproducible example set.seed(123) d1 &lt;- rnorm(1000) d2 &lt;- rnorm(1000) # Here&#39;s where my starts # let&#39;s create some custom descriptive # stats summary for the data generated above d1_mean &lt;- mean(d1) d1_sd &lt;- sd(d1) d1_q &lt;- quantile(d1) desc_stats_d1 &lt;- list(d1_mean = d1_mean, d1_sd = d1_sd, d1_q = d1_q) d2_mean &lt;- mean(d2) d2_sd &lt;- sd(d2) d2_q &lt;- quantile(d2) desc_stats_d2 &lt;- list(d2_mean = d2_mean, d2_sd = d2_sd, d2_q = d2_q) To function defintions and calls like that # Imagine you had thousand of datasets. # Imagine you wanted to add some other stats # Imagine all the error prone c&amp;p with # the above solution. # Think of how much easier this is to document. # This is automation. Not cooking. create_basic_desc &lt;- function(distr){ out &lt;- list( mean = mean(distr), sd = sd(distr), quantiles = quantile(distr) ) out } create_basic_desc(d1) ## $mean ## [1] 0.01612787 ## ## $sd ## [1] 0.991695 ## ## $quantiles ## 0% 25% 50% 75% 100% ## -2.809774679 -0.628324243 0.009209639 0.664601867 3.241039935 create_basic_desc(d2) ## $mean ## [1] 0.04246525 ## ## $sd ## [1] 1.009674 ## ## $quantiles ## 0% 25% 50% 75% 100% ## -3.04786089 -0.65322296 0.05485238 0.75345037 3.39037082 Start to document functions and their parameters using Roxygen syntax and you‚Äôre already very close to creating your first package. Pro-tipp: Hit Cmd+Alt+Shift+R3 while inside a function definition with you cursor. When working with R Studio, it will create a nifty roxygen skeleton with all your function‚Äôs parameters. #&#39; Create Basic Descriptive Statistics #&#39; #&#39; Creates means, standard deviations and default quantiles from an numeric input vector. #&#39; #&#39; @param distr numeric vector drawn from an arbitraty distribution. #&#39; @export create_basic_desc &lt;- function(distr){ out &lt;- list( mean = mean(distr), sd = sd(distr), quantiles = quantile(distr) ) out } Writing reusable code will improve your ability to remember syntax and apply concepts to other problems. The more you do it, the easier and more natural becomes. Just like a toddler figuring out how to speak in a natural language. At first progress seems small, but once kids understand the bits and pieces of a language they start building at a remarkable speed, learn and never forget again. 4.6 Plan Your Program How much planning ahead is optimal for your project ultimately depends on your experience, number of collaborators and size of your project. But still, a rough standard checklist helps any project. 4.6.1 Documentation First things first. Write the first bit of documentation before your first line of code. Documentation written with hindsight will always be written with an all-knowing, smartest-person-in-the-room mindset and the motivation of someone who already gave her best programming. Understand, I am not talking about the finetuning here, but about a written outline. Describe how parts of the code are going to do stuff. Also, examples can‚Äôt hurt to illustrate what you meant. Research projects often take breaks and getting back to work after months should be as easy as possible. Pseudo Code is a good way of writing up such an outline documentation. Take a simple API wrapper for example. Assume there is an API that returns numeric ids of hit entries when queried for keywords. These ids can be passed on to yet another endpoint, to obtain a profile. A rough game plan for an API Wrapper could like this: # function: keyword_search(keyword, url = &quot;https://some.default.url.com&quot;) # returns numeric ids according to some api documentation # function: query_profile(vec_in_ids) # a json object that should be immediately turned into list by the function, # returns list of properties Documentation should use your ecosystem‚Äôs favorite documentation framework. Yet, your comments within the code are the raw, initial form of documentation. Comments help to understand key parts of a program as well as caveats. Comments help tremendously during development time, when debugging or coming back to a project. Let alone when joining a project started by others. While pseudo code where comments mimmick code itself is the exception to that rule, good comments should always follow the not-what-but-why principle. Usually, most high level programming languages are fairly easy to read and remind of rudimentary English. Therefore a what comment like this is considered rather useless: # compute the cumulative sum of a vector cumsum(c(T,F,F,F,F,T,F,F,T,F,F,F,T)) Whereas this why comment may actually be helpful: # use the fact that TRUE is actually stored as 1 # to create a sequence until the next true # this is useful for splitting the data later on. cumsum(c(T,F,F,F,F,T,F,F,T,F,F,F,T)) Comment on why you do things, especially with which plan for future use in mind. Doing so will certainly foster exchange with others who enter or re-visit the code at a later stage (including yourself). 4.6.2 Design Your Interface In other languages it is fairly common to define the data type of both: the input and the output4. Though doing so is not necessary in R, it is good practice to define the types of all parameters and results in your comments / documentation. Once you know a bit more about your direction of travel, it‚Äôs time to think about how to modularize your program. How do different parts of the program play together. users interact with your program: Will your code just act as a storage pit of tools, a loose collection of commands for adhoc use? Are others using the program, too? Will there be machine-to-machine interaction? Do you need graphical user interface (GUI) like shiny? These questions will determine whether you use a strictly functional approach, a rudimentary form of object orientation like S3, a stricter implementation like R6 or something completely exotic. There a plenty of great resources out there, so I will not elaborate on this for the time being. The main message of this section is: Think about the main use case. Is it interactive? Is it a program that runs in batch typically? Do your users code? Would they prefer a GUI? 4.6.3 Dependencies One important word of advice for novice package developers is to think about your dependencies. Do not take depedencies lightly. Of course it is intriguiging to stand on the shoulders of giants. Isn‚Äôt R great because of its 15K+ extension packages? Isn‚Äôt exactly this was made R such as popular language? Yes, extension packages are cool. Yes, the ease with with CRAN packages are distributed is cool. But, just because packages are easy to install and free of license costs it does not mean leaning on a lot of packages comes at no costs: One needs to stay informed about updates, issues, breaking changes or undesired interdependencies between packages. The problem is mitigated a bit when a) a package is required in an interactive script and b) one is working with a very popular package. Well managed packages with a lot of reverse dependencies tend to deprecate old functionality more smoothly as authors are aware of the issues breaking changes cause to a package‚Äôs ecosystem. In R, the tidyverse bundle of packages seems ubiquitous and easy to use. But it leads to quite a few dependencies. The data.table ecosystem might be less popular but provides its functionality with a single R package dependency (the {methods} package). Often it does not take much to get rid of dependency: library(stringr) cow &lt;- &quot;A cow sounds off: mooooo&quot; str_extract(cow,&quot;moo+&quot;) ## [1] &quot;mooooo&quot; Sure, the above code is more intuitive, but shrewd use of good ol‚Äô gsub and back referencing allows you to do the very same thing in base R. gsub(&quot;(.+)(mooo+)&quot;,&quot;\\\\2&quot;,cow) ## [1] &quot;mooooo&quot; Again, {stringr} is certainly a well crafted package and it is definitely not the worst of all packages. But when you just loaded a package because it adds convenience to one single line or worse just because you found your solution online, think again before adding more dependencies to a production environment. 4.7 Naming Conventions: Snake, Camel or Kebap Before we start with files and folders, let me drop a quick, general note on naming. As in how to name files, folders and functions. It may look like a mere detail, but concise formatting and styling of your code will be appreciated by your peers and by those you ask for help. Plus, following an established convention will not make you look like a complete greenhorn. Do NOT use spaces in folder or file names! Never. If you need lengthy descriptions, use underscores ‚Äô_‚Äò, dashes‚Äô-‚Äô or camelCase. avoid umlauts and special characters. Encoding and internationalization is worth a book of its own. It‚Äôs not like modern programming environments can‚Äôt handle it, but encoding will introduce further complications. These are exactly the type of complications that may lead to an unplanned, frustrating waste of hours. You may be lucky enough to find a quick fix, but you may as well not. Avoid encoding issues if do not plan to build a deeper understanding of encoding on the fly. This is especially true for cross platform collaboroations (Windows vs.¬†Unix / OSX). either go for camelCase, snake_case or kebap-case. Otherwise prefer lower case characters. Also make sure to not switch styles within a project. There a plenty of style guides around, go with whatever your lab or community goes. 4.8 Folder Structure In R, packages may have the following folders. Note that this does not mean a package has to contain all of these folders. FWIW, an R package needs to have NAMESPACE and DESCRIPTION files, but that is not the point here. Also there are more comprehensive, better books on the matter than this little section. The point of this section though is to discuss the role of folders and how they help you structure your work, even if you don‚Äôt want to create an R package in first place. This chapter describes the role of different folders in a package and what these folders are good for. More likely than not, this will cover a lot of the aspects of your project, too. R data docs vignettes src inst man The below description explains the role of all of these folders. R A folder to store function definitions as opposed to function calls. Typically every function goes into a separate file. Sometimes it makes sense to group multiple functions into a single file when functions are closely related. Another reason for putting more than one function into a single file is when you have a collection of relatively simple, short helper functions. The R folder MUST NOT contain calls5. my_func_def &lt;- function(param1, param2){ # here goes the function body, i.e., what the function does a &lt;- (param1 + param2) * param3 # Note that in R, return statements are not necessary and even # relatively uncommon, R will return the last unassigned statement return(a) } man This folder contains the context manual of your package. What you‚Äôll find here is the so called function reference, basically a function and dataset specific documentation. It‚Äôs what you see when you run ?function_name. The content of the man/ folder is usually created automatically from the roxygen style documentation (note the #‚Äô styled comments) during a `devtools::document() run. Back in the days when people wore pijamas and lived life slow, the man folder was filled up manually with some LaTeX reminiscant .rd files, but ever since R Studio took over in 2012, most developers use Roxygen and render the function reference part of the documentation from their comments. #&#39; Sum of Parameters Multiplied by First Input #&#39; #&#39; This functions only exists as a show case. #&#39; It&#39;s useless but nevertheless exported to the NAMESPACE of this #&#39; package so users can see it and call the function by it&#39;s name. #&#39; #&#39; @param param1 numeric input #&#39; @param param2 numeric input #&#39; @export my_func_def &lt;- function(param1, param2){ # here goes the function body, i.e., what the function does a &lt;- (param1 + param2) * param1 # Note that in R, return statements are not necessary and even # relatively uncommon, R will return the last unassigned statement return(a) } docs This folder is typically not filled with content manually. When pushed to github a docs folder can easily be published as website using Github Pages. With Github pages you can host a decently styled modern website for free. Software projects often use GitHub Pages to market a product or project or simply for documentation purposes. All you need to do is check a couple of options inside the Github Web GUI and make sure the docs/ folder contains .md or .html files as well as stylesheets (.css). The latter may sound a bit like Latin to people without a basic web development background, but there is plenty of help. The R ecosystem offers different flavors of the same idea: use a bit of markdown + R to generate website code. There is blogdown for your personal website or blog. There is pkgdown for your packages documentation. And there is even bookdown to write an online book like this. Write the markdown file, render it as HTML into the docs folder, push the docs folder to GitHub. Done. Your website will be online at username.github.io/reponame. Here‚Äôs a an example of a package down website: https://mbannert.github.io/timeseriesdb/ data If you have file based data like .csv, .RData, .json or even .xlsx put them in here. Keeping data in a separate folder inside the project directory helps to keep reference to the data relative. There is nothing more greenhorn than r read.csv(\"C:\\mbannert\\My Documents\\some_data.csv\"). Even if you like this book, I doubt you have a folder named ‚Äòmbannert‚Äô on your computer. Ah, an in case you wondered, extensive use of setwd() is even worse. Keep your reference to data (and functions alike) relative. If you are sourcing data from a remote NAS drive as it is common at many universities, you can simply mount this drive to you folder (LTMGTFY: How to mount a network drive Windows / OSX). vignettes Admittedly not the most intuitive names for a folder that is supposed to contain articles. Vignettes are part of the documentation of a good package. It‚Äôs kind of a description as if you were to write a paper about your package, including some examples of how to use it. For modern packages, vignettes are often part of their package down based online documentation. Feel free, to name this folder differently, though sticking to the convention will make it easier to turn your project into a project at a later stage. This folder typically contains Markdown or RMarkdown files. src The source folder is just here for the sake of completeness and is not needed in projects that only involve R source code. It‚Äôs reserved for those parts of a package that need compilation, e.g., C++ or FORTRAN source code. inst When you install an R package using install.packages() it will be installed in some deep dungeon on your computer where R lives within your OS. The inst/ folder allows you to ship non R files with your installation. The files of the inst folder will just be copied into the package root folder inside your installation of that package. inst is also a great place to store experimental function calls or playground files once the package ambitions become more concrete and those type of files do not live conveniently in the project root anymore. Also. I sometimes put shiny apps for local use into the inst/ folder if I want to make them part of a package. GitHub Desktop, Atlassian‚Äôs Source Tree and Tortoise are some of the most popular choices if you are not a console person.‚Ü©Ô∏é On Windows / Linux use Ctrl instead of Cmd.‚Ü©Ô∏é See statically typed language vs.¬†dynamically typed language.‚Ü©Ô∏é Essentially, examples are calls, too. Note, I do recommend to add examples. Hadley Wickham‚Äôs guide to document functions within packages shows how to add examples correctly.‚Ü©Ô∏é "],
["infrastructure.html", "5 Infrastructure 5.1 Benchmarking 5.2 Your Local Machine / localhost (It‚Äôs NOT always about Computing Power!) 5.3 Where Should I Host? 5.4 Software as a Service (SaaS) 5.5 Virtual Machines 5.6 Docker (Containers)", " 5 Infrastructure Yes, there is a third area besides your research and carpentry level programming that I suppose you should get an idea of. Again, you don‚Äôt have to master hosting servers or even clusters, but a decent overview and an idea of when to use what will help you tremendously to plan ahead. Admittedly, unless your favorite superhero is Inspector Gadget or you just always had a knack for Arduinos, Raspberry Pis or the latest beta version of the software you use, infrastructure may be the one area researcher perceive as overhead. Once I was trouble shooting a case in which a colleague was unhappy with the speed gain of using the university‚Äôs heralded cluster. It turned out he had passed his behemoth of a computation problem on to the login node of the ETH cluster. Unsuprisingly, the bouncer was not particularly good at math. Before you laugh: Do you know how many cores, how much disk space or RAM your local notebook has? I‚Äôve seen many bright researchers who were not able to tell. Note, I am not writing this to make you feel bad or to put you in front of another fat rock. There is a reason why we have sysadmins, database admins and other specialists. I am not asking researchers to become one of those. A lot of developers are lacking a decent understanding of infrastructure, too. While specialization has been the story of the last decades, there is a strong case for hybrid profiles. In software development, DevOps is such a sought after hybrid profile: A software developer with knowledge of operations or a server operator who is able to develop a program. The following section intends to share insights that help you set up a playground for trying out new things, to get a rough idea of computation hours needed or simply to put you in better position to discuss your needs with your organization‚Äôs High Performancing Computing (HPC) group. 5.1 Benchmarking Benchmarking refers to observation of computation performance in order to evaluate the efficiency of a program or simply to assess the computation hours needed. Also some programs benefit from parallel computing a lot more than others and the effect of speed gains is not always linear. Being able to set up a basic toy example and get a rough idea how long a computation might take is a very valuable skill for an empirical researcher. And it‚Äôs up for grabs. Really. 5.2 Your Local Machine / localhost (It‚Äôs NOT always about Computing Power!) A simple benchmark can help to assess whether your local machine, i.e., your desktop or notebook computer may be sufficient for your ‚Äòbig‚Äô data project. Maybe some overnight computation with home court advantage is the better solution than an away game in a cloud environment you never worked with before. Also, it‚Äôs important to understand that it is not always about computing power. Your local computer can be a great testing and development environment for many things, e.g., a report rendering service, a website and many other things that might run elsewhere in production. Many of today‚Äôs applications run on some kind of webserver or in a web based architecture. Remember: a server does not necessarily look like a fridge with emergency power supply. It‚Äôs just a program that listens. Such a program can run on a fat rack and your little notebook alike which allows you to test an application locally. The popular blog software hugo for example, ships with a little webserver to test things at home before going public. hugo serve When run inside a folder with a hugo based website, the command spins up a local webserver and exposes the current state of the hugo based website through a specific port (1313). This allows the developer to visit the site using their favorite web browser while its running on their local machine. 5.3 Where Should I Host? But what if you do not want to run your stuff locally. Maybe because you want to expose a web application or site which has to be online 24/7. Maybe because you do not more additional computing power. In principle there are four options. Bare Metal. Buy hardware, hook it up with the internet, install your software, have fun. If you have a consistantly high usage rate and can maintain the server efficiently, this is likely the cheapest computing hour. You saw that if, didn‚Äôt you? Either use physical or virtual hardware and make your local IT department host your service on-premise (inhouse). You will give up a little bit of your flexible in exchange for help with maintenance and server administration. Cloud Computing. Software-as-a-Service 5.3.1 Cloud Computing Cloud computing is the on-demand of hosting. It continues to be popular with users looking for storage and computing power, particularly when users have high peaks of usage and low and long valleys of idle time. From a business perspective, cloud computing is one of the most compelling business cases of the past few decades. Economies of scale are just tremendous for companies with right (=massive) infrastructure. Amazon who happened to have the infrastructure due to its global retail business developed into one of the leading cloud computing companies turning hosting into one of its fastest growing an most profitable branches. For the users, cloud computing is just convenient. Yet, pound-for-pound, if used 24/7 cloud computing ressources are more expensive than traditional hosting. Also, limiting your budget is not trivial especially if you a sharing the costs with multiple stakeholders inside your organization. 5.4 Software as a Service (SaaS) 5.5 Virtual Machines Just like any software, tools like hugo have their dependencies. Depending on your operating system and the software you want to run, dependencies may become an issue. Dependencies may cause conflicts with other software or dependencies, e.g., some software might depend on Python 2 while another piece depends on Python 3 and so forth. The more stuff you install, the higher the chances you run into a conflict or some hard to install piece of software. One good solution to these issues are Virtual Machines. Virtual Machines can either run locally using software like Oracle‚Äôs Virtual Box which allows to run a Virtual Windows or Linux inside Mac OS and vice versa. Running a virtual box locally may not be the most performant solution but it allows to have several test environments without altering one‚Äôs main environment. In the meantime though, Virtual Machines are mostly known as flexible computation nodes offered by (cloud) hosting providers. Automated, scripted procedures to set and spin up these virtual servers make Virtual Machines a very powerful, reproducible and scaleable approach. 5.6 Docker (Containers) At the first glimpse containers look very much like Virtual Machines to the practitioner. The difference is that every Virtual Machine has its own operating system while containers use the the host OS to run a container engine on to top of the OS. By doing so containers can be very light weight and may take only a few seconds to spin up while spinning up Virtual Machines can take up to a few minutes - just like booting physical computers. Hence docker containers are often used as single purpose environments: Fire up a container, run a task in that environment, store the results outside of the container and shut down the container again. Docker the most popular containerized solution quickly became the synonym to environments configured in a file. So called docker images are build layer by layer based on other less specific docker images and a DOCKERFILE that holds the ingredients and set up for the next image. The platform dockerhub hosts a plethora of pre-built docker images vom ready-to-go database to Python ML environments or minimal Linux containers to a simple shell script. Though a bit out of fashion and somewhat different, virtual machines are good starting point to explain single purpose container environments such as docker. A virtual machine (VM) is basically a computer in a computer, like a Linux environment running on your Windows notebook. Oracle‚Äôs free Virtual Box is the most popular piece of free software to easily install another operating system inside your local computer‚Äôs host OS. While VMs are still common and one can potentially have lots of images for different purposes, images are too heavyweight and take to long to boot to be the go-to solution for many application developers. Hence so-called containers that run within a container host and fire up within seconds have become popular as single purpose environments. The most popular of them all is docker which allows users to configure an environment in a Dockerfile (essentially a text file) including the operating system and software packages installed in the container. The text file can either be used to create a docker image which is kind of a blueprint for a container. Containers run inside a docker host and can either be used interactively or in a batch which executes a single task in an environment specifically built for this task. One of the reasons why docker is attractive to researchers is its open character: Dockerfiles are a good way to share a configuration in a simple, reproducible script, making it easy to reproduce. Less experienced researchers can benefit from Dockerhub which shares images for a plethora of purposes from mixed data science setups to database configuration. Side Effect free working environments for all sorts of task can especially be appealing to developers with limited experience in system administration. Beside simplification of system administration, docker is known for its ability to work in the cloud. All major cloud hosters offer docker hosts and the ability to deploy docker containers that were previously developed and tested locally. You can also use docker to tackle throughput problems using container orchestration tools like Docker Swarm or K8 (say: Kubernetes) to run hundreds of containers (depending on your virtual resources). "],
["case-studies.html", "6 Case Studies 6.1 Consuming APIs 6.2 Create Your Own API 6.3 A Minimal Webscraper: Extracting Publication Dates 6.4 Automate Script Execution: A GitHub Actions Example 6.5 Articles, Presentations, Reports and Websites /w Markdown and RMarkdown 6.6 Web Application /w the Shiny Framework 6.7 Spatial Visualization with Leaflet and Open Streetmap 6.8 Basic Parallel Programming", " 6 Case Studies While the rest of the book provided more of a big picture type of insight, this section is all about application minded examples that feature code to reproduce. 6.1 Consuming APIs An Application Programming Interface (API) is nothing else but an interface to facilitate machine to machine communication. An interface can be anything, any protocol or pre-defined process. But of course there are standard and not-so-standard ways to communicate. Plus some matter-of-taste type of decisions. But security and standard compliance are none of the latter. There are standards such as the popular, URL based REST that make developers‚Äô lives a lot easier ‚Äì regardless of the language they prefer. Many services such as Google Cloud, AWS, your university library, your favorite social media platform or your local metro operator provide an API. Often either the platform itself or the community provide what‚Äôs called an API wrapper: A simple program wraps the process of using the interface though dynamic URLs into a parameterized function. Because the hard work is done serverside by the API backend, building API wrappers is fairly easy and if you‚Äôre lucky wrappers for your favorite languages exit already. If that is the case end users can simply use functions like get_dataset(dataset_id) to download data programatically. 6.1.1 Example 1: The {kofdata} R package The KOF Swiss Economic Institute at ETH Zurich provides such a wrapper in an R package. The underlying API allows to access the KOF time series archive database and obtain data and meta information alike. The below code snippet gets data from the API and uses another KOF built library ({tstools}) to visualize the returned time series. library(kofdata) # just for viz library(tstools) tsl &lt;- get_time_series(&quot;ch.kof.barometer&quot;) tsplot(tsl) 6.1.2 Example 2: The {OECD} R package Also large organizations like the Organization for Economic Co-operation and development (OECD) provide API wrappers to facilitate data consumption. 6.1.3 Build Your Own API Wrapper Here‚Äôs an example of a very simple API wrapper that makes use of the Metropolitan Museum of Modern Art‚Äôs API to obtain identifiers of pictures based on a simple search. # Visit this example query # https://collectionapi.metmuseum.org/public/collection/v1/search?q=umbrella # returns a json containing quite a few ids of pictures that were tagged &#39;umbrella&#39; #&#39; Search MET #&#39; #&#39; This function searches the MET&#39;s archive for keywords and #&#39; returns object ids of search hits. It is a simple wrapper #&#39; around the MET&#39;s Application Programming interface (API). #&#39; The function is designed to work with other API wrappers #&#39; and use object ids as an input. #&#39; @param character search term #&#39; @return list containing the totoal number of objects found #&#39; and a vector of object ids. #&#39; # Note these declaration are not relevant when code is not # part of a package, hence you need to call library(jsonlite) # in order to make this function work if you are not building # a package. #&#39; @examples #&#39; search_met(&quot;umbrella&quot;) #&#39; @importFrom jsonlite formJSON #&#39; @export search_met &lt;- function(keyword){ # note how URLencode improves this function # because spaces are common in searches # but are not allowed in URLs url &lt;- sprintf(&quot;https://collectionapi.metmuseum.org/public/collection/v1/search?q=%s&quot;, URLencode(keyword)) fromJSON(url) } You can use these ids with another endpoint in order to receive the pictures themselves. download_met_images_by_id &lt;- function(ids, download = &quot;primaryImage&quot;) { # Obtain meta description objects from MET API obj_list &lt;- lapply(ids, function(x) { req &lt;- download.file(sprintf(&quot;https://collectionapi.metmuseum.org/public/collection/v1/objects/%d&quot;, x),destfile = &quot;temp.json&quot;) fromJSON(&quot;temp.json&quot;) }) # Extract the list elements that contains # img URLs in order to pass it to the download function img_urls &lt;- lapply(obj_list, &quot;[[&quot;, download) # Note the implicit return, no return statement needed # last un-assigned statement is returned from the function lapply(seq_along(img_urls), function(x) { download.file(img_urls[[x]], destfile = sprintf(&quot;data/image_%d.jpg&quot;, x) ) }) } # Step 4: Use the Wrapper umbrella_ids &lt;- search_met(&quot;umbrella&quot;) umbrella_ids download_met_images_by_id(umbrella_ids$objectIDs[2:4]) 6.2 Create Your Own API Being able to expose data is a go-to skill in order to make research reproducible and credible. Especially when data get complex and require thorough description in order to remain reproducible for others, a programmtic, machine readable approach is the way to go. 6.2.1 GitHub to Serve Static Files Exposing your data through an API is not something you would need a software engineer or an own server infrastructure for. Simply hosting a bunch of .csv spreadsheet alongside a good description (in separate files!!) on, e.g., GitHub for free can be an easy highly available solution to serve static files. The KOF High Frequency Economic Monitoring dashboard simply shares standardized .csv (data) and .json (description) files based on a Github. INSERT SCREENSHOT OF GITHUB HERE To make it look at little niftier, the dashboard uses a quasar frontend to guide the human user, but it would not be necessary to have such a framework. INSERT SCREENSHOT OF KOFDATA HERE 6.2.2 Simple Dynamic APIs Even going past serving static files, does not require much software development expertise. Thanks to frameworks such as express.js or the {plumbr} it easy to create an API that turns a URL into a server side action and returns a result. Assume you‚Äôve installed node.js already, you can set up a simple API on your local computer just like this. # run initialization in a dedicated folder mkdir api cd api npm init just sleep walk through the interactive dialog accepting all defaults. Once done, add install express using the npm package manager. npm install express --save 6.3 A Minimal Webscraper: Extracting Publication Dates Even though KOF Swiss Economic Institute offers a REST API to consume publicly available data, publication dates are unfortunately not available through in API just yet. Hence, in order to automate data consumption based on varying publication dates, we need to extract upcoming publication dates of the Barometer from KOF‚Äôs media release calendar. Fortunately all future releases are presented online an easy-to-scrape table. So here‚Äôs the plan: Use Google Chrome‚Äôs inspect element developer feature to find the X-Path (location in the Document Object Model) of the table. Read the web page into R using rvest. Copy the X-Path string to R to turn the table into a data.frame use a regular expression to filter the description for what we need. Let‚Äôs take a look at our starting point, the media releases sub page, first. The website looks fairly simple and the jackpot is not hard, presented in a table right in front of us. Can‚Äôt you smell the data.frame already? Right click the table to see a Chrome context window pop up. Select inspect. Hover over the blue line in the source code at the bottom. Make sure the selected line marks the table. Right click again, select copy -&gt; copy X-Path. On to R! library(rvest) # URL of the media release subsite url &lt;- &quot;https://kof.ethz.ch/news-und-veranstaltungen/medien/medienagenda.html&quot; # Extract the DOM object from the path we&#39;ve previously detected using # Chrome&#39;s inspect feature table_list &lt;- url %&gt;% read_html() %&gt;% html_nodes(xpath = &#39;//*[@id=&quot;contentMain&quot;]/div[2]/div/div[3]/div/div/div/div/div/div/table&#39;) %&gt;% # turn the HTML table into an R data.frame html_table() # because the above result may potentially contain multiple tables, we just use # the first table. We know from visual inspection of the site that this is the # right table. agenda_table &lt;- table_list[[1]] # extract KOF barometer lines pub_date &lt;- agenda_table[grep(&quot;barometer&quot;,agenda_table$X3),] pub_date ## X1 X2 X3 X4 X5 ## 5 30. Sep. 9:00 KOF¬†Konjunkturbarometer Kalendereintrag ## 12 29. Okt. 9:00 KOF¬†Konjunkturbarometer Kalendereintrag ## 18 30. Nov. 9:00 KOF¬†Konjunkturbarometer Kalendereintrag ## 24 30. Dez. 9:00 KOF¬†Konjunkturbarometer Kalendereintrag Yay! We got everything we wanted. Ready to process. 6.4 Automate Script Execution: A GitHub Actions Example 6.5 Articles, Presentations, Reports and Websites /w Markdown and RMarkdown Working with data asks for reproducible reports, presentations and articles that can be rendered to accessible, screenreader friendly online output as well as more traditional, print minded PDF output. This case study explains how to create documents like this book, a presentation for a conference or simple, static reporting minded websites. Markdown is simple language based approach that can handle all of these tasks and allows for automation at the same time. Automation of document rendering does not only save time, it also ensures reproducibility across different machines. The name Markdown is a wordplay as Markdown is a simple mark-up language, similar to HTML or LaTeX but with a much much simpler syntax. Markdown helps users to format plain text using reserved characters. For example, a set of ** at the beginning and at the end of a word, sentence or paragraph lets everything wrapped in between these double asterisk appear bold, wrapping text in a single * asterisk makes the wrapped text italic. Markdown has become so ubiquitous that websites such as GitHub, discussion boards or Q&amp;A sites allow users who post online to use Markdown to format their text. Various rendering engines can render Markdown into HTML, PDF or even MS Word output. Often pandoc is a popular driver as a universal document converter under the hood. Programming languages such as Python and R have embraced Markdown based reporting to mingle descriptive plain text with code in order to execute the code at render time and fit tables, figures and even animations into reports. Jupyter notebooks is an interactive Python based example. In the R ecosystem the approach is slightly different and is called knitting which is why the corresponding package is called knitr. In the meantinem there is a plethora of packages (with great documentation that I won‚Äôt c&amp;p here but refer to) for different flavors of output: {bookdown} for online books such as this one, {pkgdown} for documenting your own R package or {blogdown} to run your own markdown based blog (hosted for free on, e.g., GitHub). In addition to these ‚Äúmain diagonal‚Äù packages, there smaller gems such as the {postcard} package to get a mini personal website up in a few minutes. Markdown source of this particular site that you are reading here. Markdown source of a presentation slides INSERT SKETCH HERE 6.6 Web Application /w the Shiny Framework Fancy dashboards and the promise of an easy to use way to create dynamic web applications have made the {shiny} R package one of the most frequent items on data people‚Äôs bucket list of things to learn. In the meantime {shiny} has gathered an entire ecosystem of helper packages, e.g., to boilerplate applications or to make use of other frontend frameworks through {shiny}. This case studies intends to de-mystify the package and to explain basic concepts, but no rewrite its great documentation or blogs and books around {shiny}. The code for a {shiny} app can live in a single file, but for the sake of illustrating the concept I will use two files here: a frontend file called ui.R and a backend file called server.R. If we follow this convention and put both files into one folder a the shiny runtime 6.6.1 Frontend: ui.R INSERT JOHNNY BRAVO HERE. MAYBE I AM LATE BUT I LOOK GOOD. In a separation-of-concerns approach, the part of the application which makes sure fonts, figures and tables are neatly presented in our web browsers is called the frontend. The markup-language HTML has been taking care of this for decades and therefore has been associated with web programming even though it is not a programming language, but rather a language that helps to format and position things ‚Äì very much like LaTeX. To make it easier to implement entire styles, e.g., corporate design, CSS has been around as HTML‚Äôs partner for several decades. The trio is completed by the Javascript programming language which went from messy scripting language to being used everywhere. In web frontends it‚Äôs mainly used to make things dynamic by modifying the so-called DOM, i.e., the tree like structure of HTML while the page is already rendered and being displayed in our browsers. So what has the {shiny} R package to do with all of this? {shiny} makes sure you do not have to care about the above if you do not want to. Abstracting HTML/CSS/JS way from the developer leads varying opinions about {shiny} which maybe very confusing for the novice. The R world mostly loves while application developers don‚Äôt get the hype (if they know the {shiny} framework at all). However, {shiny} is a great opportunity to create beautiful state-of-the-art web applications w/o learning a set of new languages if you come out of of the data corner. As part of this process {shiny} helps developers organize their code into backend and frontend code, something not necessarily common for people for rather program with data as opposed to application developers. A graphical user interfaces (GUI) let alone user experience (UX) is a beast of its own. A frontend ui.R file could look like this library(shiny) 6.6.2 Backend: server.R The server.R files does all the heavy lifting. Computation, file processing, data access it all happens inside the server function. Sometimes you want to change the user interfaces depending on the result of some computation. In this special case the user interface 6.6.3 Quickstart: A Minimal Demo App If want to you catch a glimpse of {shiny} running on your own computer asap, simply install the {shiny} R package and run the following single file application. library(shiny) ui &lt;- fluidPage( mainPanel( plotOutput(&quot;rndtest&quot;), sliderInput(&quot;rnd_n&quot;,&quot;Number of Observations&quot;,1,1000,100) ) ) server &lt;- function(input, output, session) { output$rndtest &lt;- renderPlot({ hist(rnorm(input$rnd_n)) }) } shinyApp(ui, server) Running the code will fire up a shiny server and a separate session on tha server to run your app on your local computer. Your development server is up an running and will react to many changes on the fly w/o having to restart the server for the changes to come into effect. 6.6.4 Hosting: Make Apps Available Online Though some shiny apps just live happily inside R packages many of them are designed to be shown to a larger public as standalone apps. Examples include for an interactive data visualization or forms to gather input data. There are many reasons to make {shiny} apps available online. In order to do so, you can either host them yourself within your university or company infrastructure or using your favorite cloud provider. The inventors of shiny, the R Studio company also offers hassle free software-as-a-service (SaaS) shiny hosting on shinyapps.io. For a more detailed discussion of on-premise (in house hosting) or service based hosting please take a look at the infrastructure section of this book. 6.7 Spatial Visualization with Leaflet and Open Streetmap 6.8 Basic Parallel Programming Here‚Äôs an R example using the {microbenchmark} package to check the effect parallel computing on running seasonal adjustment of multiple time series. First let‚Äôs set up some demo data: We simply create a list with 1000 elements each of which is the same monthly time series about airline passengers from 1949-1960. data(&quot;AirPassengers&quot;) tsl &lt;- list() for(i in 1:1000){ tsl[[i]] &lt;- AirPassengers } Now, let‚Äôs load the {seasonal} package and perform a basic seasonal adjustment of each of these time series. The first statement performs 1000 adjustments sequentially, the second statement uses parallel computing to spread computations across my machines multiple processors. The key take away from this exercise is not the parallel computation itself, but the ability to set up a benchmark. library(seasonal) library(microbenchmark) Obviously the absolute computation time depends on the hardware used, but also the operating system can be an important factor depending on the task at hand. Even though the combination of algorithm, hardware, operating system and software used for the computation can make assessment a daunting, complex task, easily venturing into expert realm, a basic understanding and a ballpark relation between multiple approaches can carry you a long way. "],
["connect-with-the-developer-community.html", "7 Connect with the Developer Community 7.1 Stay up to Date and a Vastly Evolving Field ‚Äì Social Media 7.2 Get an Account at Stackoverflow.com 7.3 Attend Conferences - Online Can Be a Good Option ! 7.4 Join Slack Spaces (or other Chats) 7.5 Look for Local Community Group", " 7 Connect with the Developer Community Besides the fact that it is free of license costs most of the time, open source communities may be the most popular argument in favor of open source software. In academia, free quality online resources and support can be extremely helpful, particularly when one‚Äôs curriculum did not contain an applied form of programming with data. Still, developer communities do have their idiosyncrasies. This section intends to help overcome entry barriers and encourage the reader to connect to the developer communities ‚Äì even if one is not a regular visitor of community offers. 7.1 Stay up to Date and a Vastly Evolving Field ‚Äì Social Media I hate to admit it, because I am not a big fan of social media and don‚Äôt use social media apart from professional use, I am convinced it is a great way to get my regular dose of what‚Äôs new in tech. My platform of choice is twitter. I use it as a bookmark tool and get some feedback from publicly sharing resources. Plus I follow some folks to get updates from different the fields. I try to not get caught in any politics, memes or cat pictures, even if that‚Äôs a hard thing to do. CHEWBACCA cat. Start with a few accounts to follow, adapt them regularly and use lists in case you need to organize your input a bit more. tweetdeck is a good standard option of a more advanced use of twitter. If you plan to build some following of your own, learn how to schedule tweets and put some thought into the timing of your tweets. However, Globally different preferences. CITE useR! nature correspondence 7.2 Get an Account at Stackoverflow.com 7.3 Attend Conferences - Online Can Be a Good Option ! 7.4 Join Slack Spaces (or other Chats) Slack or Discord 7.5 Look for Local Community Group job market! "],
["appendix.html", "Appendix Glossary", " Appendix Glossary Term Description API Application Programming Interface CamelCase Convention to spell file, variable or function names reminiscant of a camel, e.g., doSomething() CMS Content Management System Console Also known as terminal, the console is an interface which takes written user commands. Bash is one of the most popular terminals on OS level, but scripting languages like Python and R have consoles to communicate with there interpreter,too. Deployment The art of delivering a piece software to production Endpoint Part of an API, a generic URL that follows a systematic that can be exploited to automate machine-to-machine data exchange. Fork a clone of a repository that you (usually) do not own. GUI Graphical User Interface IDE Integrated Development Environment Kebap Case Spelling convention less known than snake case and camel case, kebap case looks like this: my-super-folder. Lexical Scoping Look up of variables in parent environments when they can‚Äôt be found in the current environment. Be aware that this is the default behavior of R. Merge Request See Pull Request. OS Operating Systen OSS Open Source Software Pull Request (PR) Request to join a feature branch into another branch, e.g., main branch. Sometimes it‚Äôs also called merge request. Regular Expression Pattern to extract specific parts from a text, find stuff in a text. REPL read-eval-print-loop Reproducible Example A self-contained code example, including the data it needs to run. Snake_case Convention to spell file, variable or function names reminiscant of a snake, e.g., do_something() Stack selection of software used in a project SQL Structured Query Language Swimlanes (Online) Board of columns (lanes). Lanes progress from from left to right and carry issues. Throughput Problem A bottleneck which can be mitigated by parallelization, e.g., multiple containers running in parallel. Transactional Database Database optimised for production systems. Such a database is good at reading and writing individual rows w/o affecting the other and while taking care of data integrity. Virtual Machine (VM) A virtual computer hosted on your computer. Often used to run another OS inside your main OS for testing purposes. "]
]
