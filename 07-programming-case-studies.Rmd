# Case Studies

While the rest of the book provided more of a big picture type of insight, this section is all about application minded examples that feature code to reproduce. 


## Consuming APIs 

An Application Programming Interface (API) is nothing else but an interface to facilitate machine to machine communication. An interface can be anything, any protocol or pre-defined process. But of course there are standard and not-so-standard ways to communicate. Plus some matter-of-taste type of decisions. But security and standard compliance are none of the latter. 
There are standards such as the popular, URL based REST that make developers'
lives a lot easier -- regardless of the language they prefer. 

Many services such as Google Cloud, AWS, your university library, your favorite social media platform or your local metro operator provide an API. Often either the platform itself or the community provide what's called an API wrapper: 
A simple program wraps the process of using the interface though dynamic URLs into a parameterized function. Because the hard work is done serverside by the API backend, building API wrappers is fairly easy and if you're lucky wrappers for your favorite languages exit already. If that is the case end users can simply use functions like `get_dataset(dataset_id)` to download data programatically. 


### Example 1: The {kofdata} R package

The KOF Swiss Economic Institute at ETH Zurich provides such a wrapper in an R package. The underlying API allows to access the KOF time series archive database and obtain data and meta information alike. The below code snippet gets data from the API and uses another KOF built library ({tstools}) to visualize the returned time series. 


```{r, warning=FALSE, message=FALSE}
library(kofdata)
# just for viz
library(tstools)
tsl <- get_time_series("ch.kof.barometer")
tsplot(tsl)

```



### Example 2: The {OECD} R package 

Also large organizations like the Organization for Economic Co-operation and development (OECD) provide API wrappers to facilitate data consumption. 

### Build Your Own API Wrapper

Here's an example of a very simple API wrapper that makes use of the 
Metropolitan Museum of Modern Art's API to obtain identifiers of pictures based on 
a simple search. 

```{r, eval=FALSE, warning=FALSE,message=FALSE}
# Visit this example query
# https://collectionapi.metmuseum.org/public/collection/v1/search?q=umbrella
# returns a json containing quite a few ids of pictures that were tagged 'umbrella'

#' Search MET
#'
#' This function searches the MET's archive for keywords and
#' returns object ids of search hits. It is a simple wrapper
#' around the MET's Application Programming interface (API).
#' The function is designed to work with other API wrappers
#' and use object ids as an input.
#' @param character search term
#' @return list containing the totoal number of objects found
#' and a vector of object ids.
#'
# Note these declaration are not relevant when code is not
# part of a package, hence you need to call library(jsonlite)
# in order to make this function work if you are not building
# a package.
#' @examples
#' search_met("umbrella")
#' @importFrom jsonlite formJSON
#' @export
search_met <- function(keyword){
  # note how URLencode improves this function
  # because spaces are common in searches
  # but are not allowed in URLs
  url <- sprintf("https://collectionapi.metmuseum.org/public/collection/v1/search?q=%s", URLencode(keyword))
  fromJSON(url)
}

```

You can use these ids with another [endpoint]() in order to receive the pictures
themselves.

```{r, eval=FALSE,warning=FALSE,message=FALSE}
download_met_images_by_id <- function(ids,
                                      download = "primaryImage") {
  # Obtain meta description objects from MET API
  obj_list <- lapply(ids, function(x) {
    req <- download.file(sprintf("https://collectionapi.metmuseum.org/public/collection/v1/objects/%d",
                   x),destfile = "temp.json")
    fromJSON("temp.json")
  })

  # Extract the list elements that contains
  # img URLs in order to pass it to the download function
  img_urls <- lapply(obj_list, "[[", download)
  # Note the implicit return, no return statement needed
  # last un-assigned statement is returned from the function
  lapply(seq_along(img_urls), function(x) {
    download.file(img_urls[[x]],
                  destfile = sprintf("data/image_%d.jpg", x)
    )
  })
}


# Step 4: Use the Wrapper
umbrella_ids <- search_met("umbrella")
umbrella_ids
download_met_images_by_id(umbrella_ids$objectIDs[2:4])
```




## Create Your Own API

Being able to expose data is a go-to skill in order to make research reproducible and credible. Especially when data get complex and require thorough description in order to remain reproducible for others, a programmtic, machine readable approach is the way to go. 


### GitHub to Serve Static Files

Exposing your data through an API is not something you would need a software engineer or an own server infrastructure for. Simply hosting a bunch of .csv spreadsheet alongside a good description (in separate files!!) on, e.g., GitHub for free can be an easy highly available solution to serve static files. 

The KOF High Frequency Economic Monitoring dashboard simply shares standardized .csv (data) and .json (description) files based on a Github. 


INSERT SCREENSHOT OF GITHUB HERE

To make it look at little niftier, the dashboard uses a quasar frontend to guide the human user, but it would not be necessary to have such a framework.

INSERT SCREENSHOT OF KOFDATA HERE


### Simple Dynamic APIs

Even going past serving static files, does not require much software development expertise. 
Thanks to frameworks such as express.js or the {plumbr} it easy to create an API that turns a URL into a server side action and returns a result. 

Assume you've installed node.js already, you can set up a simple API on your local computer just like this. 

```
# run initialization in a dedicated folder
mkdir api
cd api
npm init
```

just sleep walk through the interactive dialog accepting all defaults. Once done, add install express using the npm package manager.

```
npm install express --save
```














## Basic Parallel Programming


## Presentations, Reports and Websites /w RMarkdown


## Web Application /w the Shiny Framework

Fancy dashboards and the promise of an easy to use way to create dynamic web applications have made the {shiny} R package one of the most frequent items on data people's bucket list of things to learn. 
In the meantime {shiny} has gathered an entire ecosystem of helper packages, e.g., to boilerplate applications or to make use of other frontend frameworks through {shiny}. This case studies intends to de-mystify the package and to explain basic concepts, but no rewrite its great documentation or blogs and books around {shiny}.

### Frontend: ui.R

INSERT JOHNNY BRAVO HERE. MAYBE I AM LATE BUT I LOOK GOOD.

In a separation-of-concerns approach, the part of the application which makes sure fonts, figures and tables are neatly presented in our web browsers is called the *frontend*. The markup-language **HTML** has been taking care of this for decades and therefore has been associated with web programming even though it is not a programming language, but rather a language that helps to format and position things -- very much like LaTeX. To make it easier to implement entire styles, e.g., corporate design, **CSS** has been around as HTML's partner for several decades. The trio is completed by the **Javascript** programming language which went from messy scripting language to being used everywhere. In web frontends it's mainly used to make things dynamic by modifying the so-called DOM, i.e., the tree like structure of HTML while the page is already rendered and being displayed in our browsers. 

So what has the {shiny} R package have to do with all of this? It makes sure you do not have to care about the above if you do not want to. This is the main reason why we hear bi-polar assessments about {shiny} which maybe very confusing for the novice. Seasoned web developers wil likely not know {shiny} or if they do will wonder why one would use some language made for statistical computing to build a website. On the other hand people who ather come out of the programming with data corner, {shiny} is great opportunity to create beautiful state-of-the-art web applications w/o learning a set of new languages. 

{shiny} takes care 





### Backend: server.R


### Hosting: Make Apps Available Online










## Spatial Visualization with Leaflet and Open Streetmap




